1. find duplicates with fdupes
2. assign each duplicate an idea (shared among duplicates, unique for each group)
3. order duplicates by their path
4. create a tree of the paths that contain duplicates

#this tree contains all folders which in turn contain duplicates
#these folders are themselves potential duplicates
#the tree is going to be huge, many folders are probably not important
#therefore the next step is meant to thin out the tree

# problem: if a node is removed from the tree, because it isn't a duplicate, what happens to its children ?
# an idea would be to just mark the node as not duplicate and keep it.

5. go through the tree with dfs:
    look at each node


    if the node is a leaf:
        if the leaf is a duplicate:
            # this is one of the duplicates that fdupes found, keep it
            keep it
        else
            # a leaf, which is not a duplicate ?, must be a folder which does not contain duplicates
            remove it


    if the node has children:
        # all children are potential duplicates, if the number of folders differs, this is not a duplicate
        if the number of files and folder in this path is different from the number of children:
            remove it
        else:
            keep it


now the tree contains a lot of meta information
we need to assign each node an id, this id consists of the duplicates contained in the folder

6. go through the tree with dfs:
    look at each node

    if the node is a duplicate:
        create checksum based on the checksum of all its children
    if the node is not a duplicate:
        enter an error value into the checksum
        # maybe -1, but I'm not sure if a checksum could generate this value