1. find duplicates with fdupes
2. assign each duplicate an idea (shared among duplicates, unique for each group)
3. order duplicates by their path
4. create a tree of the paths that contain duplicates

#this tree contains all folders which in turn contain duplicates
#these folders are themselves potential duplicates
#the tree is going to be huge, many folders are probably not important
#therefore the next step is meant to thin out the tree

! instead of removing: a flag is set that this cant be a duplicate

5. go through the tree with dfs:
    look at each node


    if the node is a leaf:
        if the leaf is a duplicate:
            # this is one of the duplicates that fdupes found, keep it
            keep it
        else
            # a leaf, which is not a duplicate ?, must be a folder which does not contain duplicates
            remove it


    if the node has children:
        # all children are potential duplicates, if the number of folders differs, this is not a duplicate
        if the number of files and folder in this path is different from the number of children:
            remove it
        else:
            keep it


now the tree contains a lot of meta information
we need to assign each node an id, this id consists of the duplicates contained in the folder

6. go through the tree with dfs:
    look at each node

    if the node is a potential duplicate:
        create checksum based on the checksum of all its children
    if the node is not a duplicate:
        enter an error value into the checksum
        # maybe -1, but I'm not sure if a checksum could generate this value

7. go through the tree with dfs:
    look at each node

    if the node is a potential duplicate:
        add its checksum to a global list of checksums

8. look at the list of checksums and find duplicates, search for them in the tree